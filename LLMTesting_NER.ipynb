{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v3fzWj8DZotF"
   },
   "source": [
    "# **Testing Named Entity Recognition (NER) Task**\n",
    "#### Tests fundamental capability: Understanding\n",
    "\n",
    "## Introduction\n",
    "\n",
    "NER (Named Entity Recognition) is a fundamental task in Natural Language Processing (NLP). It involves identifying and categorizing named entities in text, like people, organizations, and locations. The main goal of NER is to assess a model's ability to classify named entities in unstructured text into predefined categories.\n",
    "\n",
    "In this tutorial, we will evaluate the Named Entity Recognition (NER) capabilities of the GPT-3.5 Turbo language model. The tutorial uses the CoNLL 2003 dataset, a benchmark for NER tasks, to assess the model's ability to identify and classify named entities such as people, organizations, and locations in text. The notebook includes code to load the dataset, interact with the GPT-3.5 Turbo model via the OpenAI API, and evaluate the model's performance by comparing its predictions to the ground truth annotations in the CoNLL dataset.\n",
    "\n",
    "\n",
    " The CoNLL dataset consists of text data with annotated named entities: Person, Organization, Location, and Miscellaneous.\n",
    "\n",
    "For additional information about the CoNLL benchmark: https://arxiv.org/pdf/cs/0306050v1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qrXPNMPKat-b"
   },
   "source": [
    "## Step 1: Install Pre-requisites\n",
    "\n",
    "In step 1, we will load the pre-requisites\n",
    "\n",
    "We need to install the following libraries:\n",
    "- `openai`: For interacting with the OpenAI API to query the LLM.\n",
    "- `python-dotenv`: To manage API keys securely using environment variables.\n",
    "- `datasets`: The datasets library provides easy access to a wide variety of datasets commonly used for natural language processing tasks.\n",
    "- `tqdm`: Adds progress bars to loops, making it easier to monitor & visualize the progress\n",
    "- `rich`: A library to render rich text (for display purposes)\n",
    "- `scikit-learn`: A ML library with implmenetations of algorithms, metrics for classification, regression, clustering tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2j1IBt3EXWBo",
    "outputId": "fd153f9c-9df5-437c-f499-7a902b3c6325"
   },
   "outputs": [],
   "source": [
    "# %pip install openai==1.102.0 python-dotenv datasets tqdm rich scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will import the necessary libraries that will be used for various activities such as data processing, API interaction, and environment management tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rK5VttK0Zvpe"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import random\n",
    "from dotenv import load_dotenv\n",
    "from rich import print as rprint\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from IPython.display import display, HTML\n",
    "from tqdm import tqdm\n",
    "import ast\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aOEp2oyTa2VP"
   },
   "source": [
    "## Step 2: Load LLM\n",
    "\n",
    "Establishing connection with LLM through API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_LIdQaqIZ0a6"
   },
   "outputs": [],
   "source": [
    "# Load API key from environment file\n",
    "# load_dotenv(dotenv_path=\"../apikey.env.txt\")  # replace the \"file path\" with the location of your API key file\n",
    "\n",
    "# APIKEY = os.getenv(\"APIKEY\")\n",
    "\n",
    "# openai.api_key = APIKEY\n",
    "\n",
    "import openai\n",
    "from openai import OpenAI\n",
    " \n",
    "client = OpenAI(\n",
    "    base_url=\"http://localhost:11434/v1\",  # Local Ollama API\n",
    "    api_key=\"ollama\"                       # Dummy key\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oF2VqaG_aD4i"
   },
   "source": [
    "A function to interact with the LLM and extract NER tokens from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lFtqh3juZ4eQ"
   },
   "outputs": [],
   "source": [
    "def GetModelResponse(system_content, user_content):\n",
    "    system = {'role': 'system', 'content': system_content}\n",
    "    user = {'role': 'user', 'content': user_content}\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gemma3:4b\",\n",
    "        messages=[system, user],\n",
    "        #max_tokens=2000\n",
    "        #temperature=1.0,\n",
    "    )\n",
    "\n",
    "    content = response.choices[0].message.content\n",
    "    return content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rg5CX5dVaVJH"
   },
   "source": [
    "## Step 3: Load test dataset\n",
    "\n",
    "The CoNLL dataset consists of articles with annotated entities like person, organization, location and miscellaneous.\n",
    "\n",
    "Next, we will download and load the CoNLL 2003 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conll_data = load_dataset(\"conll2003\", revision=\"refs/convert/parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UQ-Hax0yaeKZ"
   },
   "source": [
    "**Exploring the CoNLL dataset**\n",
    "\n",
    "The CoNLL dataset consists of three parts: a training set, a validation set, and a test set.\n",
    "\n",
    "We will use the test dataset to perform NER evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qJdLXmXOahji",
    "outputId": "6fb517d6-ea49-4553-9611-e26ba3e9ab4e"
   },
   "outputs": [],
   "source": [
    "# Display basic statistics about the dataset\n",
    "train_data = conll_data['train']\n",
    "test_data = conll_data['test']\n",
    "valid_data = conll_data['validation']\n",
    "\n",
    "print(f\"Training set size: {len(train_data)}\")\n",
    "print(f\"Validation set size: {len(valid_data)}\")\n",
    "print(f\"Test set size: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4gUFVWhybBtM"
   },
   "source": [
    "**The test set consists of 3453 instances**.\n",
    "\n",
    "As a tester, you have the option to select the number of instances (out of 3453) on which the LLM will be evaluated. **If no options are provided, the script will default to evaluating 25 instances**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "2R4VYJKwbc0w",
    "outputId": "c28c6fba-1da8-4ce5-f07c-167aaf013907"
   },
   "outputs": [],
   "source": [
    "# Ask user for the number of test instances to evaluate\n",
    "num_test_instances = input(\"Enter the number of test instances to evaluate (default 25): \")\n",
    "num_test_instances = int(num_test_instances) if num_test_instances else 25\n",
    "\n",
    "# Select the random test instances\n",
    "random.seed(12)\n",
    "test_instances = random.sample(list(test_data), num_test_instances)\n",
    "\n",
    "rprint(f\"[blue]Number of test instances: {len(test_instances)}[/blue]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pWSJNnrFhTGJ"
   },
   "source": [
    "## Step 4: Prompt Construction\n",
    "\n",
    "We will construct the prompt to instruct the model to identify and classify entities from the input text. Then, we will evaluate the model performance by comparing the predicted label with the ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eTLHOppQiglF"
   },
   "outputs": [],
   "source": [
    "# Map numeric tags to string labels for CoNLL dataset\n",
    "label_mapping = {\n",
    "    0: 'O',\n",
    "    1: 'B-PER',\n",
    "    2: 'I-PER',\n",
    "    3: 'B-LOC',\n",
    "    4: 'I-LOC',\n",
    "    5: 'B-ORG',\n",
    "    6: 'I-ORG',\n",
    "    7: 'B-MISC',\n",
    "    8: 'I-MISC'\n",
    "}\n",
    "\n",
    "def get_ner(tokens):\n",
    "    # Convert tokens to a string format that's cleaner\n",
    "    tokens_str = str(tokens)\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gemma3:4b\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a named entity recognition system. You must respond with ONLY a valid Python dictionary where keys are tokens and values are NER tags. No explanations, no additional text, just the dictionary.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"\"\"Tokens: {tokens_str}\n",
    "\n",
    "Tag each token with one of: O, B-PER, I-PER, B-LOC, I-LOC, B-ORG, I-ORG, B-MISC, I-MISC\n",
    "\n",
    "Example format: {{'John': 'B-PER', 'lives': 'O', 'in': 'O', 'London': 'B-LOC'}}\n",
    "\n",
    "Response format (dictionary only):\"\"\"}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    ner_results = response.choices[0].message.content.strip()\n",
    "    \n",
    "    # Remove any markdown formatting\n",
    "    ner_results = re.sub(r'```python\\s*', '', ner_results)\n",
    "    ner_results = re.sub(r'```\\s*', '', ner_results)\n",
    "\n",
    "    # Fix - to remove \"json\" prefix\n",
    "    ner_results = re.sub(r'^json\\s*\\n?', '', ner_results)\n",
    "    ner_results = ner_results.strip()\n",
    "    \n",
    "    try:\n",
    "        # Use ast.literal_eval for safe evaluation\n",
    "        ner_dict = ast.literal_eval(ner_results)\n",
    "        if isinstance(ner_dict, dict):\n",
    "            # Ensure all tokens are covered, default to 'O' if missing\n",
    "            result = {}\n",
    "            for token in tokens:\n",
    "                result[token] = ner_dict.get(token, 'O')\n",
    "            return result\n",
    "    except (ValueError, SyntaxError) as e:\n",
    "        print(f\"Parsing error: {e}\")\n",
    "        print(f\"Raw response: {ner_results}\")\n",
    "        # Return default tags for all tokens\n",
    "        return {token: 'O' for token in tokens}\n",
    "    \n",
    "    return {token: 'O' for token in tokens}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K-QcZc08sHnE"
   },
   "source": [
    "Next, we'll evaluate the LLM's performance by comparing its predicted NER tags against the ground truth labels.\n",
    "\n",
    "Correctly predicted tokens will be highlighted in green, while mismatches will be displayed in orange."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "pUwwyP7-jjhg",
    "outputId": "558aef92-385b-45d8-dfc8-75fb1be2be35"
   },
   "outputs": [],
   "source": [
    "# Compare LLM predictions with ground truth\n",
    "predictions = []\n",
    "ground_truths = []\n",
    "\n",
    "for instance in test_instances:\n",
    "    tokens = instance['tokens']\n",
    "    ground_truth = {tokens[i]: label_mapping[instance['ner_tags'][i]] for i in range(len(tokens))}\n",
    "    prediction = get_ner(tokens)\n",
    "\n",
    "    # print(tokens)\n",
    "    # print(ground_truth)\n",
    "    # print(prediction)\n",
    "\n",
    "    predictions.append(prediction)\n",
    "    ground_truths.append(ground_truth)\n",
    "\n",
    "    display_text = []\n",
    "    for token in tokens:\n",
    "        pred_tag = prediction.get(token, 'O')\n",
    "        gt_tag = ground_truth[token]\n",
    "        if pred_tag == gt_tag:\n",
    "            display_text.append(f\"<span style='color: green;'>{token}</span>\")\n",
    "        else:\n",
    "            display_text.append(f\"<span style='color: orange;'>{token}</span>\")\n",
    "\n",
    "    display(HTML(f\"<b>Input Text:</b> {' '.join(display_text)}\"))\n",
    "    display(HTML(f\"<b>Ground Truth:</b> {ground_truth}\"))\n",
    "    display(HTML(f\"<b>Prediction:</b> {prediction}\"))\n",
    "    display(HTML(\"<hr>\"))\n",
    "\n",
    "# Function to convert NER tags to a comparable format\n",
    "def convert_to_comparable_format(ner_dict):\n",
    "    return [(word, tag) for word, tag in ner_dict.items()]\n",
    "\n",
    "# Calculate precision, recall, and F1-score\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for gt, pred in zip(ground_truths, predictions):\n",
    "    gt_converted = convert_to_comparable_format(gt)\n",
    "    pred_converted = convert_to_comparable_format(pred)\n",
    "\n",
    "    for word, tag in gt_converted:\n",
    "        y_true.append(tag)\n",
    "        y_pred.append(pred.get(word, 'O'))  # 'O' for non-entity\n",
    "\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted', zero_division=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finally, print out results.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Results summary\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
